---
title: "Allen_707_HW8"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages('randomForest')
#install.packages('stringr')
library(tidyverse)
```

```{r}
require(caret)
require(e1071)
require(rpart)
require(dplyr)
require(stringr)
require(randomForest)
library(rsample)
```

```{r}
## Data Preparation

#First load the training data in csv format, and then convert "Survived" to nominal variable and "Pclass" to ordinal variable.

digit_train <- read.csv("digit-train.csv")
digit_test <- read.csv("digit-test.csv")

digit_train$label <- as.factor(digit_train$label)
digit_test$label <- as.factor(digit_test$label)


```

```{r}
search_grid = expand.grid(k = c(5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25))

# set up 3-fold cross validation procedure
train_control <- trainControl(
  method = "cv", 
  number = 3
  )

# more advanced option, run 5 fold cross validation 10 times
train_control_adv <- trainControl(
  method = "repeatedcv", 
  number = 3,
  repeats = 10
  )

# train model
knn <- train(label ~ .,
  data = digit_train,
  method = "knn",
  trControl = train_control_adv,
  tuneGrid = search_grid
  )



```

```{r}
knn$results %>% 
  top_n(5, wt = Accuracy) %>%
  arrange(desc(Accuracy))

# results for best model
confusionMatrix(knn)

pred <- predict(knn, data = digit_test)

#Average accuracy of .9212 for KNN

#In this case, there are no unnecesary attributes, so KNN is likely a top model choice
```

```{r}
#install.packages("kernlab")
library(tidyverse)    # data manipulation and visualization
library(kernlab)      # SVM methodology
library(e1071)        # SVM methodology
library(RColorBrewer) # customized coloring of plot
```

```{r}
search_grid = expand.grid(C = seq(0, 2, length = 20))

# set up 3-fold cross validation procedure
train_control_2 <- trainControl(
  method = "cv", 
  number = 3
  )

# more advanced option, run 5 fold cross validation 10 times
train_control_adv2 <- trainControl(
  method = "repeatedcv", 
  number = 3,
  repeats = 10
  )


svm.m1 = train(label ~., data = digit_train, 
      method = "svmLinear", 
      trControl = train_control_2,
      tuneGrid = search_grid)

# top 5 models
svm.m1$results %>% 
  top_n(5, wt = Accuracy) %>%
  arrange(desc(Accuracy))

# results for best model
confusionMatrix(svm.m1)

pred2 <- predict(svm.m1, data = digit_test)

#Average accuracy of .9026 for SVM Linear model, slightly lower than KNN

#Data isn't too noisy, so this model may not be the most efficient method 

```

```{r}
# for reproduciblity
set.seed(123)

# default RF model
m1 <- randomForest(
  formula = label ~ .,
  data = digit_train,
  ntree = 500,
  mtry  = 2
)

m1

plot(m1)

#used this plot for reference of random forest capabilities
```


```{r}
search_grid = expand.grid(.mtry = (1:5)) 

# set up 3-fold cross validation procedure
train_control <- trainControl(
  method = "cv", 
  number = 3
  )

# more advanced option, run 5 fold cross validation 10 times
train_control_adv <- trainControl(
  method = "repeatedcv", 
  number = 3,
  repeats = 10
  )


rf.m1 = train(label ~., data = digit_train, 
      method = "rf",
      metric = 'Accuracy',
      trControl = train_control_adv,
      tuneGrid = search_grid)

# top 5 modesl
rf.m1$results %>% 
  top_n(5, wt = Accuracy) %>%
  arrange(desc(Accuracy))

# results for best model
confusionMatrix(rf.m1)

pred <- predict(rf.m1, newdata = digit_train)

#All three algorithms produce high prediction accuracy
#Random forest subsets may prove to be most valuable
```

